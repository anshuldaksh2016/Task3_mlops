# -*- coding: utf-8 -*-
"""Building an Artificial Neural Network in TensorFlow 2.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tj02zPuUkkbLob1oo63f9TliYj5DQHA3



## Stage 1: Installing dependencies and setting up GPU environment
"""

!pip install tensorflow-gpu==2.0.0.alpha0

"""## Stage 2: Import dependencies for the project"""

import numpy as np
#import datetime
import tensorflow as tf
from tensorflow.keras.datasets import fashion_mnist

tf.__version__

"""## Stage 3: Dataset preprocessing

### Loading the dataset
"""

#Loading the Fashion Mnist dataset
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

"""### Image normalization

We devide each image in the training and testing dataset with the maxiumum number of pixels (255).

In this way each pixel will be in the rainge [0, 1]. By normalizing imaes we are making sure that our model (ANN) trains faster.
"""

X_train = X_train / 255.0

X_test = X_test / 255.0

"""### Reshaping of the dataset

Since we are using fully connected network, we reshape the training and testing subsets to be in the vector format.
"""

#Since each image is 28x28, we simply use reshape the full dataset to [-1 (all elements), height * width]

X_train = X_train.reshape(-1, 28*28)

X_train.shape

#Reshape the testing subset in the same way
X_test = X_test.reshape(-1, 28*28)

"""## Stage 4: Building an Artificial Neural network

### Defining the model

Simply define an object of the Sequential model.
"""

model = tf.keras.models.Sequential()

"""### Adding the first layer (Dense layer)

Layer hyper-parameters:
- number of units/neurons: 128
- activation function: ReLU
- input_shape: (784, )
"""

model.add(tf.keras.layers.Dense(units=128, activation='relu', input_shape=(784, )))

"""### Adding a Dropout layer 

Dropout is a Regularization technique where we randomly set neurons in a layer to zero. In this way, while training those neurons won't be updated. Because some percentage of neurons won't be updated the whole training process is long and we have less chance for overfitting.
"""

model.add(tf.keras.layers.Dropout(0.2))

"""### Adding the second layer (output layer)

- units == number of classes (10 in the case of Fashion MNIST)
- activation = 'softmax'
"""

model.add(tf.keras.layers.Dense(units=10, activation='softmax'))

"""### Comiling the model

- Optimizer: Adam
- Loss: Sparse softmax (categorical) crossentropy
"""

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])

model.summary()

"""### Training the model"""

model.fit(X_train, y_train, epochs=5)

"""### Model evaluation and prediction"""

test_loss, test_accuracy = model.evaluate(X_test, y_test)

print("Test accuracy: {}".format(test_accuracy))

accuracy = test_accuracy[1]*100

# storing accuracy

import os

os.system("sudo touch    accuracy.txt")
os.system("echo {} >   /accuracy.txt".format(accuracy))

# saving the model

model.save("fashion_mnist.h5")
